<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs">
  <meta name="description" content="We introduce Adversarial Prompt Distillation (APD), a novel framework that transfers jailbreak capabilities from large language models (LLMs) to small language models (SLMs) using masked language modeling, KL divergence, reinforcement learning, and dynamic temperature control. APD achieves high attack success rates with significantly reduced time and resource demands.">
  <meta name="keywords" content="jailbreak attacks, adversarial prompt distillation, LLMs, SLMs, knowledge distillation, reinforcement learning, LLM security, prompt engineering">
  <meta name="author" content="Xiang Li, Chong Zhang, Jia Wang, Fangyu Wu, Yushi Li, Xiaobo Jin">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Xi’an Jiaotong-Liverpool University">
  <meta property="og:title" content="Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs">
  <meta property="og:description" content="We introduce Adversarial Prompt Distillation (APD), a novel framework that transfers jailbreak capabilities from large language models (LLMs) to small language models (SLMs) using masked language modeling, KL divergence, reinforcement learning, and dynamic temperature control. APD achieves high attack success rates with significantly reduced time and resource demands.">
  <meta property="og:url" content="https://franz-chang.github.io/Adversarial-Prompt-Distillation">
  <meta property="og:image" content="https://franz-chang.github.io/Adversarial-Prompt-Distillation/static/images/apd_overview.png">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs">
  <meta name="twitter:description" content="We introduce Adversarial Prompt Distillation (APD), a novel framework that transfers jailbreak capabilities from large language models (LLMs) to small language models (SLMs) using masked language modeling, KL divergence, reinforcement learning, and dynamic temperature control. APD achieves high attack success rates with significantly reduced time and resource demands.">
  <meta name="twitter:image" content="https://franz-chang.github.io/Adversarial-Prompt-Distillation/static/images/apd_overview.png">

  <!-- Academic Specific -->
  <meta name="citation_title" content="Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs">
  <meta name="citation_author" content="Li, Xiang">
  <meta name="citation_author" content="Zhang, Chong">
  <meta name="citation_author" content="Wang, Jia">
  <meta name="citation_author" content="Wu, Fangyu">
  <meta name="citation_author" content="Li, Yushi">
  <meta name="citation_author" content="Jin, Xiaobo">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_pdf_url" content="https://franz-chang.github.io/Adversarial-Prompt-Distillation/static/pdfs/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt_Distillation_from_LLMs_to_SLMs.pdf">
  
  <title>Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs</title>
  
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
</head>
<body>

  <!-- Hero -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://github.com/lxgem" target="_blank">Xiang Li</a><sup>*</sup>,</span>
              <span class="author-block">Chong Zhang<sup>*</sup>,</span>
              <span class="author-block">Jia Wang,</span>
              <span class="author-block">Fangyu Wu,</span>
              <span class="author-block">Yushi Li,</span>
              <span class="author-block"><a href="mailto:xiaobo.jin@xjtlu.edu.cn">Xiaobo Jin</a><sup>†</sup></span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Xi’an Jiaotong-Liverpool University,<br>
              <sup>2</sup>The Chinese University of Hong Kong,<br>
              <sup>3</sup>University of Liverpool</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution <sup>†</sup>Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered mt-5">
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/pdfs/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt_Distillation_from_LLMs_to_SLMs.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/lxgem/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt/tree/main" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://franz-chang.github.io/Adversarial-Prompt-Distillation" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-globe"></i></span><span>Project Page</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Figure 1: Complexity -->
  <section class="hero is-small">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <figure class="image">
          <img src="static/images/apd_overview.png" alt="Complexity of mainstream jailbreak methods">
        </figure>
        <h2 class="subtitle has-text-centered"><strong>Figure 1:</strong> Measuring the complexity of mainstream generative jailbreaks (time & memory). APD drastically reduces both.</h2>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>As the scale and complexity of jailbreaking attacks on large language models (LLMs) continue to escalate, their efficiency and practical applicability are constrained, posing a profound challenge to LLM security. Recent advances have automated jailbreaking approaches that harness LLMs to generate jailbreak instructions and adversarial examples, delivering encouraging results. Nevertheless, these methods universally incorporate an LLM generation phase, which impedes their effective implementation.</p>
            <p>We introduce <strong>Adversarial Prompt Distillation (APD)</strong>, an innovative framework that integrates masked language modeling, reinforcement learning, and dynamic temperature control to distill LLM jailbreaking prowess into smaller language models (SLMs). This enables efficient, robust jailbreak attacks while maintaining high success rates. Empirical evaluations affirm APD’s superiority in attack efficacy, resource optimization, and cross-model versatility.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Key Contributions -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Key Contributions</h2>
      <div class="columns is-multiline is-centered">
        <div class="column is-half">
          <div class="box"><div class="content"><h4>First to transfer jailbreak capability from LLMs to SLMs</h4><p>Enables lightweight models (e.g., BERT) to generate highly effective adversarial prompts.</p></div></div>
        </div>
        <div class="column is-half">
          <div class="box"><div class="content"><h4>Adversarial Prompt Distillation (APD) Framework</h4><p>Multi-stage pipeline combining LoRA fine-tuning, KL divergence, dynamic temperature control, and RLAIF-based template selection.</p></div></div>
        </div>
        <div class="column is-half">
          <div class="box"><div class="content"><h4>State-of-the-art efficiency & performance</h4><p>Outperforms GCG, AutoDAN, TAP, LLM-Virus in ASR while reducing time/memory by orders of magnitude.</p></div></div>
        </div>
        <div class="column is-half">
          <div class="box"><div class="content"><h4>Important security implications</h4><p>Reveals critical vulnerabilities in current LLM safety and enables scalable red-teaming on edge devices.</p></div></div>
        </div>
      </div>
    </div>
  </section>

  <!-- Methodology -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Methodology</h2>
      <figure class="image">
        <img src="static/images/figure2_apd_framework.png" alt="APD Framework Overview" loading="lazy">
      </figure>
      <h2 class="subtitle has-text-centered"><strong>Figure 2:</strong> Overview of Adversarial Prompt Distillation (APD).</h2>

      <div class="columns is-variable is-8">
        <div class="column">
          <h3 class="title is-4">Template Selection</h3>
          <p>We select the top-10 jailbreak templates according to a composite score of <strong>stealthiness</strong>, <strong>harmfulness</strong>, <strong>efficiency</strong>, and <strong>diversity</strong>.</p>
        </div>
        <div class="column">
          <h3 class="title is-4">Two-Stage Distillation</h3>
          <ul>
            <li><strong>Pre-training:</strong> Enhance Llama-3.x as teacher with harmful generation ability</li>
            <li><strong>Knowledge Distillation + RL:</strong> Transfer capability to BERT/ALBERT/RoBERTa using KL divergence, dynamic temperature annealing, and RLAIF policy optimization</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <!-- Experiments -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Experiments</h2>

      <h3 class="title is-4">Main Results on AdvBench & HarmBench</h3>
      <div class="columns">
        <div class="column">
          <p class="has-text-centered"><strong>Table 4 – AdvBench (ASR<sub>k</sub> / ASR<sub>l</sub>)</strong></p>
          <table class="table is-bordered is-striped is-fullwidth">
            <thead><tr><th>Method</th><th>GPT-4</th><th>GPT-3.5</th><th>Llama-2-7B</th><th>Vicuna-7B</th></tr></thead>
            <tbody>
              <tr><td>LLM-Virus</td><td>74.0 / 36.5</td><td>90.8 / 96.5</td><td>95.6 / 96.6</td><td>93.5 / 97.0</td></tr>
              <tr><td><strong>APD (Ours)</strong></td><td><strong>96.4 / 69.2</strong></td><td><strong>99.7 / 99.6</strong></td><td><strong>96.1 / 96.6</strong></td><td><strong>100.0 / 99.6</strong></td></tr>
            </tbody>
          </table>
        </div>
        <div class="column">
          <p class="has-text-centered"><strong>Table 5 – HarmBench (ASR<sub>c</sub>)</strong></p>
          <table class="table is-bordered is-striped is-fullwidth">
            <thead><tr><th>Method</th><th>GPT-4-0613</th><th>GPT-3.5-Turbo</th><th>Llama-2-7B</th><th>Vicuna-7B</th></tr></thead>
            <tbody>
              <tr><td>LLM-Virus</td><td>29.3</td><td>53.8</td><td>38.5</td><td>80.5</td></tr>
              <tr><td>Ours (Teacher)</td><td>54.5</td><td>54.4</td><td>42.9</td><td>85.7</td></tr>
              <tr><td><strong>Ours (Student)</strong></td><td><strong>63.0</strong></td><td><strong>56.2</strong></td><td><strong>50.0</strong></td><td><strong>86.2</strong></td></tr>
            </tbody>
          </table>
        </div>
      </div>

      <h3 class="title is-4">Efficiency (Table 3)</h3>
      <p>After distillation, the student model (BERT) is <strong>~70× smaller</strong> and <strong>3–4× faster</strong> than the Llama teacher while achieving higher attack success rates.</p>

      <h3 class="title is-4">Key Takeaways</h3>
      <ul>
        <li>APD achieves <strong>new SOTA</strong> on both AdvBench and HarmBench across GPT-4, GPT-3.5, Llama-2, and Vicuna.</li>
        <li>Distillation not only preserves but <strong>often improves</strong> attack performance.</li>
        <li>Enables efficient jailbreak attacks on resource-constrained devices.</li>
      </ul>
    </div>
  </section>

  <!-- BibTeX -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{li2025efficient,
  title   = {Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs},
  author  = {Li, Xiang and Zhang, Chong and Wang, Jia and Wu, Fangyu and Li, Yushi and Jin, Xiaobo},
  journal = {arXiv preprint arXiv:2506.17231},
  year    = {2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>Built with the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template</a>.</p>
      </div>
    </div>
  </footer>
</body>
</html>
